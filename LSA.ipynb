{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA是一种自然语言处理中用到的方法，其通过“矢量语义空间”来提取文档与词中的“概念”，进而分析文档与词之间的关系。**LSA的基本假设是，如果两个词多次出现在同一文档中，则这两个词在语义上具有相似性。**LSA使用大量的文本上构建一个矩阵，这个矩阵的一行代表一个词，一列代表一个文档，矩阵元素代表该词在该文档中出现的次数，然后再此矩阵上使用奇异值分解（SVD）来保留列信息的情况下减少矩阵行数，之后每两个词语的相似性则可以通过其行向量的cos值（或者归一化之后使用向量点乘）来进行标示，此值越接近于1则说明两个词语越相似，越接近于0则说明越不相似。\n",
    "\n",
    "LSA 使用**词-文档矩阵**来描述一个词语是否在一篇文档中。词-文档矩阵式一个稀疏矩阵，其行代表词语，其列代表文档。一般情况下，词-文档矩阵的元素是该词在文档中的出现次数，也可以是是该词语的tf-idf(term frequency–inverse document frequency)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构建好词-文档矩阵之后，LSA将对该矩阵进行**降维**，来找到词-文档矩阵的一个低阶近似。\n",
    "\n",
    "**降维的原因**\n",
    "- 原始的词-文档矩阵太大导致计算机无法处理，从此角度来看，降维后的新矩阵式原有矩阵的一个近似。\n",
    "- 原始的词-文档矩阵中有噪音，从此角度来看，降维后的新矩阵是原矩阵的一个去噪矩阵。\n",
    "- 原始的词-文档矩阵过于稀疏。原始的词-文档矩阵精确的反映了每个词是否“出现”于某篇文档的情况，然而我们往往对某篇文档“相关”的所有词更感兴趣，因此我们需要发掘一个词的各种同义词的情况。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于矩阵X，X<sub>ij</sub>,代表词语i在文档j中的出现次数，每一行描述了该词和所有文档的关系。一列代表一个文档向量，该向量描述了该文档与所有词的关系。点乘可以表示这两个单词在文档集合中的相似性。矩阵XX<sup>T</sup>包含所有词向量点乘的结果。\n",
    "\n",
    "**通常对X做SVD分解。**\n",
    "\n",
    "X=UWV<sup>T</sup>\n",
    "\n",
    "当我们选择k个最大的奇异值，和它们对应的Ｕ与Ｖ中的向量相乘，则能得到一个X矩阵的**k阶近似**，此时该矩阵和X矩阵相比有着最小误差（即残差矩阵的Frobenius范数）。但更有意义的是这么做可以将**词向量和文档向量映射到语义空间**。i.e.是一个高维空间到低维空间的近似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "低维的语义空间可以用于以下几个方面:\n",
    "\n",
    "- 在低维语义空间可对文档进行比较，进而可用于文档聚类和文档分类。\n",
    "- 在翻译好的文档上进行训练，可以发现不同语言的相似文档，可用于跨语言检索。\n",
    "- 发现词与词之间的关系，可用于同义词、歧义词检测。.\n",
    "- 通过查询映射到语义空间，可进行信息检索。\n",
    "- 从语义的角度发现词语的相关性，可用于“选择题回答模型”（multi choice qustions answering model）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSA的缺点：**\n",
    "\n",
    "- 新生成的矩阵的解释性比较差.比如\n",
    "{(car), (truck), (flower)} ↦ {(1.3452 * car + 0.2828 * truck), (flower)}\n",
    "\n",
    " (1.3452 * car + 0.2828 * truck) 可以解释成 \"vehicle\"。同时，也有如下的变换\n",
    " \n",
    "{(car), (bottle), (flower)} ↦ {(1.3452 * car + 0.2828 * bottle), (flower)}造成这种难以解释的结果是因为SVD只是一种数学变换，并无法对应成现实中的概念。\n",
    "\n",
    "- LSA无法扑捉一词多意的现象。在原始词-向量矩阵中，每个文档的每个词只能有一个含义。比如同一篇文章中的“The Chair of Board\"和\"the chair maker\"的chair会被认为一样。在语义空间中，含有一词多意现象的词其向量会呈现多个语义的平均。相应的，如果有其中一个含义出现的特别频繁，则语义向量会向其倾斜。\n",
    "\n",
    "\n",
    "- LSA具有词袋模型的缺点，即在一篇文章，或者一个句子中**忽略词语的先后顺序**。\n",
    "\n",
    "\n",
    "- LSA的概率模型假设文档和词的分布是服从**联合正态分布**的，但从观测数据来看是服从**泊松分布**的。因此LSA算法的一个改进PLSA使用了多项分布，其效果要好于LSA。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
