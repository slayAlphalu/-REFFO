{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word 2 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand. \n",
    "\n",
    "It does so in one of two ways, either using context to predict a target word (CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets.\n",
    "\n",
    "NLP 里面，最细粒度的是 **词语**，词语组成句子，句子再组成段落、篇章、文档。所以处理 NLP 的问题，首先就要拿词语开刀。\n",
    "\n",
    "判断一个词的词性，是动词还是名词。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种\n",
    "\n",
    "- 如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』\n",
    "\n",
    "- 而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』\n",
    "\n",
    "### ohe to encode characters\n",
    "![sg](skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用x预测y，我们利用OHE将text变为numerical变量送入数学模型进行训练。例如，全世界有v个词语，“奶”和“盖”的顺序分别是第一个和第二个，那么“奶“对应的向量就是[1,0,0...,0]而”盖“对应的向量就是[0,1,0....,0]。\n",
    "输入x，我们得到一个y，y代表了这个V个词的输出概率，我们希望最好和真实的ohe一样。\n",
    "\n",
    "当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『奶』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 $V_{x}$ 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 $V_{x}$ 就可以用来唯一表示 x。\n",
    "![eg1](eg1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Skip-Gram Model\n",
    " The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\n",
    "![eg2](eg2.png)\n",
    "### The CBOW Model\n",
    "\n",
    "\n",
    "### SKIP GRAM VS CBOW\n",
    "![VS](vs.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
