{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非负矩阵分解(non-negative matrix factorization，以下简称NMF)是一种非常常用的矩阵分解方法，它可以适用于很多领域，比如图像特征识别，语音识别等，这里我们会主要关注于它在文本主题模型里的运用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 奇异值分解，它会将一个矩阵分解为三个矩阵：X=UWV<sup>T\\<sup>,\n",
    "    \n",
    "    降到k维，则表达式为：X <sub>mxn</sub> = U <sub>mxk</sub> W<sub>kxk </sub> V<sub>kxn</sub>\n",
    "\n",
    "- NMF将矩阵分解为两个矩阵:X=W <sub>mxk</sub> H<sub>kxn </sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF的优化思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF期望找到这样的两个矩阵W,H，使矩阵乘积得到的矩阵对应的每个位置的值和原矩阵X\n",
    "对应位置的值相比误差尽可能的小。用数学的语言表示就是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$arg\\min \\ {1/2}*\\sum_{i,j}( X_{ij}-(WH)_{ij})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF矩阵分解如何运用到我们的主题模型呢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们输入的有m个文本，n个词，而$X_{ij}$\n",
    "对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，$W_{ij}$\n",
    "对应第i个文本的和第k个主题的概率相关度，而$H_{Kj}$对应第j个词和第k个主题的概率相关度。\n",
    "　　　　\n",
    "\n",
    "注意到这里我们使用的是\"**概率相关度**\"，这是因为我们使用的是\"非负\"的矩阵分解，这样我们的W,H矩阵值的大小可以用概率值的角度去看。从而可以得到文本和主题的概率分布关系。第二种解释用一个图来表示如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和LSA相比，我们不光得到了文本和主题的关系，还得到了直观的概率解释，同时分解速度也不错。当然NMF由于是两个矩阵，相比LSA的三矩阵，NMF不能解决词和词义的相关度问题。这是一个小小的代价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF](NMF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1,1,5,2,3], [0,6,2,1,1], [3, 4,0,3,1], [4, 1,5,6,3]])\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.67371185 0.02013017]\n",
      " [0.40564826 2.17004352]\n",
      " [0.77627836 1.5179425 ]\n",
      " [2.66991709 0.00940262]]\n",
      "[[1.32014421 0.40901559 2.10322743 1.99087019 1.29852389]\n",
      " [0.25859086 2.59911791 0.00488947 0.37089193 0.14622829]]\n"
     ]
    }
   ],
   "source": [
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "print (W)\n",
    "print (H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可以看出， 第1,3,4,5个文本和第一个隐含主题更相关，而第二个文本与第二个隐含主题更加相关。如果需要下一个结论，我们可以说，第1,3,4,5个文本属于第一个隐含主题，而第二个问题属于第2个隐含主题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
