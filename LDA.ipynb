{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一个函数：gamma函数\n",
    "- 四个分布：二项分布、多项分布、beta分布、Dirichlet分布\n",
    "- 一个概念和一个理念：共轭先验和贝叶斯框架\n",
    "- 两个模型：pLSA、LDA\n",
    "- 一个采样：Gibbs采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人类是怎么生成文档的呢？\n",
    "\n",
    "定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练，获取每个主题Topic对应的词语。然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的重复这两步，最终生成如下图所示的一篇文章（其中不同颜色的词语分别对应上图中不同主题下的词）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lda](LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而当我们看到一篇文章后，往往喜欢推测这篇文章是如何生成的，我们可能会认为作者先确定这篇文章的几个主题，然后围绕这几个主题遣词造句，表达成文。\n",
    "\n",
    "LDA就是要干这事：根据给定的一篇文档，反推其主题分布。\n",
    "\n",
    "通俗来说，可以假定认为人类是根据上述文档生成过程写成了各种各样的文章，现在某小撮人想让计算机利用LDA干一件事：你计算机给我推测分析网络上各篇文章分别都写了些啥主题，且各篇文章中各个主题出现的概率大小（主题分布）是啥。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从狄利克雷分布$\\alpha$中取样生成文档 i 的主题分布 $\\theta_{i}$\n",
    "- 从主题的多项式分布$\\theta_{i}$中取样生成文档i 的第 j 个词的主题$z_{ij}$\n",
    "- 从狄利克雷分布$\\beta$中取样生成主题$z_{ij}$对应的词语分布$\\phi z_{ij}$\n",
    "- 从词语的多项式分布$\\phi z_{ij}$中采样最终生成词语$w_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lda](LDA2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dirichlet分布，是beta分布在高维度上的推广。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思路：**\n",
    "\n",
    "一个函数：gamma函数\n",
    "\n",
    "四个分布：二项分布、多项分布、beta分布、Dirichlet分布\n",
    "\n",
    "外加一个概念和一个理念：共轭先验和贝叶斯框架\n",
    "\n",
    "两个模型：pLSA、LDA（文档-主题，主题-词语）\n",
    "\n",
    "一个采样：Gibbs采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLSA 和 LDA 的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PLSA中，生成文档的方式如下：\n",
    "\n",
    "1. 按照概率$p(d_i)$选择一篇文档$d_i$\n",
    "\n",
    "2. 根据选择的文档$d_i$，从从主题分布中按照概率$p(\\zeta_k \\mid d_i)$选择一个隐含的主题类别$\\zeta_k$\n",
    "\n",
    "3. 根据选择的主题$\\zeta_k$, 从词分布中按照概率$p(\\omega_j \\mid \\zeta_k)$选择一个词$\\omega_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA 中，生成文档的过程如下：\n",
    "\n",
    "1. 按照先验概率$p(d_i)$选择一篇文档$d_i$\n",
    "\n",
    "2. 从Dirichlet分布$\\alpha$中取样生成文档$d_i$的主题分布$\\theta_i$，主题分布$\\theta_i$由超参数为$\\alpha$的Dirichlet分布生成\n",
    "\n",
    "3. 从主题的多项式分布$\\theta_i$中取样生成文档$d_i$第 j 个词的主题$z_{i, j}$\n",
    "\n",
    "4. 从Dirichlet分布$\\beta$中取样生成主题$z_{i, j}$对应的词语分布$\\phi_{z_{i, j}}$，词语分布$\\phi_{z_{i, j}}$由参数为$\\beta$的Dirichlet分布生成\n",
    "\n",
    "5. 从词语的多项式分布$\\phi_{z_{i, j}}$中采样最终生成词语$\\omega_{i, j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，LDA 在 PLSA 的基础上，为主题分布和词分布分别加了两个 Dirichlet 先验。\n",
    "\n",
    "可以看出PLSA中，主题分布和词分布都是唯一确定的。但是，在LDA中，主题分布和词分布是不确定的，LDA的作者们采用的是贝叶斯派的思想，认为它们应该服从一个分布，主题分布和词分布都是多项式分布，因为多项式分布和狄利克雷分布是共轭结构，在LDA中主题分布和词分布使用了Dirichlet分布作为它们的共轭先验分布。所以，也就有了一句广为流传的话 -- LDA 就是 PLSA 的贝叶斯化版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB采样\n",
    "Gibbs抽样方法是 Markov Chain Monte Carlo（MCMC）方法的一种，也是应用最为广泛的一种。\n",
    "\n",
    "马尔可夫链有个重要性质就是n次迭代后能够收敛达到稳态。而马尔可夫采样过程就是每次根据转移概率生成采样矩阵 最终达到稳态。普通的采样过程有个拒绝概率，说的是本次采样不可信。gb采样是优化的采样方法，使得每次采样都被认可，提高了采样效率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
